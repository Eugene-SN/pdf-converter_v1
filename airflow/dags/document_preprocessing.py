# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô DAG 1: Document Preprocessing
# –£—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
# - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ use_ocr —Ñ–ª–∞–≥–∞ 
# - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è processing_options
# - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø–æ—Ç–µ—Ä—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
# - –î–æ–±–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

import json
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.decorators import apply_defaults

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
from shared_utils import (
    DocumentProcessorOperator,
    SharedUtils,
    NotificationUtils,
    ConfigUtils
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
DEFAULT_ARGS = {
    'owner': 'pdf-converter',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'document_preprocessing',
    default_args=DEFAULT_ARGS,
    description='‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô DAG 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤',
    schedule_interval=None,  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
    max_active_runs=3,
    catchup=False,
    tags=['pdf-converter', 'preprocessing', 'docling', 'ocr', 'fixed']
)

def validate_input_task(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞"""
    input_file = context['dag_run'].conf.get('input_file')
    
    if not input_file:
        raise ValueError("–ù–µ —É–∫–∞–∑–∞–Ω –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")
    
    if not SharedUtils.validate_input_file(input_file):
        raise ValueError(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ñ–∞–π–ª: {input_file}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞
    import os
    file_stats = os.stat(input_file)
    
    metadata = {
        'file_path': input_file,
        'file_size_bytes': file_stats.st_size,
        'file_size_mb': round(file_stats.st_size / (1024*1024), 2),
        'file_name': os.path.basename(input_file),
        'validation_timestamp': datetime.now().isoformat()
    }
    
    print(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—Ä–æ–π–¥–µ–Ω–∞: {metadata}")
    return metadata

def prepare_processing_config(**context):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    dag_conf = context['dag_run'].conf
    
    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
    processing_config = {
        # ‚úÖ –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: OCR –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        'use_ocr': dag_conf.get('use_ocr', False),  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é FALSE –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö PDF
        'ocr_languages': dag_conf.get('ocr_languages', 'chi_sim,chi_tra,eng,rus'),
        'ocr_engine_primary': 'paddleocr',
        'ocr_engine_secondary': 'tesseract',
        'high_quality_ocr': dag_conf.get('high_quality_ocr', True),
        
        # Docling –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        'docling_device': dag_conf.get('docling_device', 'cuda'),
        'layout_analysis': True,
        'structure_detection': True,
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        'extract_tables': dag_conf.get('extract_tables', True),
        'extract_images': dag_conf.get('extract_images', True),
        'extract_formulas': dag_conf.get('extract_formulas', True),
        'extract_code_blocks': True,
        
        # Tabula-Py –¥–ª—è —Ç–∞–±–ª–∏—Ü
        'tabula_enabled': True,
        'tabula_pages': 'all',
        'tabula_lattice': True,
        'tabula_stream': True,
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        'quality_level': dag_conf.get('quality_level', 'high'),
        'preserve_formatting': True,
        'preserve_structure': dag_conf.get('preserve_structure', True),
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        'chinese_optimization': True,
        'traditional_simplified_conversion': True,
        'language': dag_conf.get('language', 'zh-CN'),
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        'max_processing_time': 1800,  # 30 –º–∏–Ω—É—Ç
        'memory_limit': '4G'
    }
    
    print(f"üìã –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   - use_ocr: {processing_config['use_ocr']}")
    print(f"   - extract_tables: {processing_config['extract_tables']}")
    print(f"   - extract_images: {processing_config['extract_images']}")
    print(f"   - language: {processing_config['language']}")
    
    return processing_config

# –ó–∞–¥–∞—á–∞ 1: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
validate_input = PythonOperator(
    task_id='validate_input_file',
    python_callable=validate_input_task,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
prepare_config = PythonOperator(
    task_id='prepare_processing_config',
    python_callable=prepare_processing_config,
    dag=dag
)

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–¥–∞—á–∞ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Document Processor
extract_content = DocumentProcessorOperator(
    task_id='extract_content',
    input_file_path="{{ dag_run.conf['input_file'] }}",
    processing_options="{{ json.dumps(task_instance.xcom_pull(task_ids='prepare_processing_config')) }}",
    timeout=1800,  # 30 –º–∏–Ω—É—Ç 
    dag=dag
)

def analyze_extraction_results(**context):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    extraction_data = context['task_instance'].xcom_pull(task_ids='extract_content')
    
    if not extraction_data:
        raise ValueError("–ù–µ –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ—Ç Document Processor")
    
    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    extracted_text = extraction_data.get('extracted_text', '')
    text_length = len(extracted_text)
    tables_count = len(extraction_data.get('tables', []))
    images_count = len(extraction_data.get('images', []))
    sections_count = len(extraction_data.get('sections', []))
    
    analysis = {
        'extraction_quality': {
            'text_extracted': text_length > 0,
            'text_length': text_length,
            'tables_found': tables_count,
            'images_found': images_count,
            'sections_found': sections_count,
            'has_structure': bool(extraction_data.get('document_structure'))
        },
        'processing_stats': extraction_data.get('processing_stats', {}),
        'readiness_for_next_dag': True
    }
    
    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    if text_length < 50:
        print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞!")
        print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: '{extracted_text[:100]}...'")
        analysis['readiness_for_next_dag'] = False
        analysis['extraction_quality']['critical_issue'] = 'insufficient_text'
    
    if text_length < 500:
        print("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ò–∑–≤–ª–µ—á–µ–Ω–æ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")
        print(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {text_length} —Å–∏–º–≤–æ–ª–æ–≤")
    
    if tables_count == 0 and 'table' in extracted_text.lower():
        print("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω—ã —Ç–∞–±–ª–∏—Ü—ã")
        analysis['extraction_quality']['possible_missed_tables'] = True
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
    dag_2_input = {
        'extracted_content': extraction_data,
        'analysis': analysis,
        'source_file': context['dag_run'].conf.get('input_file'),
        'processing_timestamp': datetime.now().isoformat(),
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        'extracted_text': extracted_text,  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        'sections': extraction_data.get('sections', []),
        'document_structure': extraction_data.get('document_structure', {}),
        'metadata': extraction_data.get('metadata', {})
    }
    
    print(f"üìä –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
    print(f"   ‚úÖ –¢–µ–∫—Å—Ç: {text_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   ‚úÖ –°–µ–∫—Ü–∏–π: {sections_count}")
    print(f"   ‚úÖ –¢–∞–±–ª–∏—Ü: {tables_count}")
    print(f"   ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {images_count}")
    print(f"   ‚úÖ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–ª—è DAG 2: {analysis['readiness_for_next_dag']}")
    
    return dag_2_input

# –ó–∞–¥–∞—á–∞ 4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
analyze_results = PythonOperator(
    task_id='analyze_extraction_results',
    python_callable=analyze_extraction_results,
    dag=dag
)

def save_intermediate_results(**context):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    dag_2_input = context['task_instance'].xcom_pull(task_ids='analyze_extraction_results')
    timestamp = context['dag_run'].conf.get('timestamp', int(datetime.now().timestamp()))
    filename = context['dag_run'].conf.get('filename', 'unknown.pdf')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
    intermediate_path = f"/tmp/dag1_results_{timestamp}.json"
    
    import json
    import os
    
    os.makedirs(os.path.dirname(intermediate_path), exist_ok=True)
    
    with open(intermediate_path, 'w', encoding='utf-8') as f:
        json.dump(dag_2_input, f, ensure_ascii=False, indent=2)
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏
    try:
        os.chown(intermediate_path, 1000, 1000)
    except PermissionError:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–º–µ–Ω–∏—Ç—å –≤–ª–∞–¥–µ–ª—å—Ü–∞ —Ñ–∞–π–ª–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤")
        pass
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è DAG 2
    next_dag_config = {
        'intermediate_file': intermediate_path,
        'original_config': context['dag_run'].conf,
        'dag1_completed': True,
        'ready_for_dag2': dag_2_input['analysis']['readiness_for_next_dag'],
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        'extraction_quality': dag_2_input['analysis']['extraction_quality'],
        'text_length': len(dag_2_input.get('extracted_text', '')),
        'processing_time': dag_2_input.get('extracted_content', {}).get('processing_time', 0)
    }
    
    print(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {intermediate_path}")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {os.path.getsize(intermediate_path)} –±–∞–π—Ç")
    
    return next_dag_config

# –ó–∞–¥–∞—á–∞ 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
save_results = PythonOperator(
    task_id='save_intermediate_results',
    python_callable=save_intermediate_results,
    dag=dag
)

def notify_completion(**context):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ DAG 1"""
    results = context['task_instance'].xcom_pull(task_ids='save_intermediate_results')
    dag_run_id = context['dag_run'].run_id
    
    if results and results.get('ready_for_dag2'):
        message = f"""
        ‚úÖ DAG 1 (Document Preprocessing) —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω
        
        Run ID: {dag_run_id}
        –§–∞–π–ª: {context['dag_run'].conf.get('filename', 'unknown')}
        
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
        - –¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω: ‚úÖ ({results.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤)
        - –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {results.get('intermediate_file')}
        - –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–ª—è DAG 2: {'‚úÖ –î–∞' if results.get('ready_for_dag2') else '‚ùå –ù–µ—Ç'}
        - –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {results.get('processing_time', 0):.2f}s
        
        –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: Content Transformation (DAG 2)
        """
    else:
        message = f"""
        ‚ùå DAG 1 (Document Preprocessing) –∑–∞–≤–µ—Ä—à–µ–Ω —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
        
        Run ID: {dag_run_id}
        –ü—Ä–æ–±–ª–µ–º–∞: {results.get('extraction_quality', {}).get('critical_issue', 'Unknown issue')}
        –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ–∫—Å—Ç–∞: {results.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤
        
        –ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
    
    print(message)
    
    # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    NotificationUtils.send_success_notification(context, results or {})

# –ó–∞–¥–∞—á–∞ 6: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
notify_task = PythonOperator(
    task_id='notify_completion',
    python_callable=notify_completion,
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∑–∞–¥–∞—á
validate_input >> prepare_config >> extract_content >> analyze_results >> save_results >> notify_task

# ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
def handle_failure(context):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤ DAG"""
    task_id = context['task_instance'].task_id
    exception = context.get('exception')
    
    error_message = f"""
    ‚ùå –û–®–ò–ë–ö–ê –í DAG 1 (Document Preprocessing)
    
    –ó–∞–¥–∞—á–∞: {task_id}
    –§–∞–π–ª: {context['dag_run'].conf.get('filename', 'unknown')}
    –û—à–∏–±–∫–∞: {str(exception)}
    
    –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:
    1. –°—Ç–∞—Ç—É—Å Document Processor: /health
    2. –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å use_ocr —Ñ–ª–∞–≥–∞
    3. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ PDF
    4. –õ–æ–≥–∏ container: docker logs document-processor
    """
    
    print(error_message)
    NotificationUtils.send_failure_notification(context, exception)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –∫–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º
for task in dag.tasks:
    task.on_failure_callback = handle_failure

import json
dag.user_defined_macros = {'json': json}